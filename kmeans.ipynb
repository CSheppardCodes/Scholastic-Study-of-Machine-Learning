{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPW8T5JrtW4W4qkqaKvPBR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSheppardCodes/Scholastic-Study-of-Machine-Learning/blob/main/kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) We are going to use the following dataset for this exercise:\n",
        "https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter\n",
        "Follow the “Data Folder” link and unzip the given file. You will file a folder containing tweets\n",
        "that contain links to various news sources e.g. the file “usnewshealth.txt” contains tweets that\n",
        "refer to articles published in US News. You have to choose one such file and proceed."
      ],
      "metadata": {
        "id": "XO-IOmTEFmon"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kt1MiUI-MgX"
      },
      "outputs": [],
      "source": [
        "import random as rd\n",
        "import re\n",
        "import math\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "#df = pd.read_csv(\"https://raw.githubusercontent.com/CSheppardCodes/MLDatasetsUCI/main/asg3/bbchealth.txt\")\n",
        "#dataurl = df.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Perform the following pre-processing steps:\n",
        "- Remove the tweet id and timestamp\n",
        "- Remove any word that starts with the symbol @ e.g. @AnnaMedaris\n",
        "- Remove any hashtag symbols e.g. convert #depression to depression\n",
        "- Remove any URL\n",
        "- Convert every word to lowercase"
      ],
      "metadata": {
        "id": "23Hy38FMDfkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_tweets(url):\n",
        "\n",
        "    f = open(url, \"r\", encoding=\"utf8\")\n",
        "    tweets = list(f)\n",
        "    list_of_tweets = []\n",
        "\n",
        "    for i in range(len(tweets)):\n",
        "\n",
        "\n",
        "        tweets[i] = tweets[i].strip('\\n')                                             # remove \\n from the end after every sentence\n",
        "        tweets[i] = tweets[i][50:]                                                    # Remove the tweet id and timestamp\n",
        "        tweets[i] = \" \".join(filter(lambda x: x[0] != '@', tweets[i].split()))        # Remove any word that starts with the symbol @\n",
        "        tweets[i] = re.sub(r\"http\\S+\", \"\", tweets[i])                                 # Remove URL\n",
        "        tweets[i] = re.sub(r\"www\\S+\", \"\", tweets[i])                                  # Remove URL\n",
        "\n",
        "        # remove colons from the end of the sentences (if any) after removing url\n",
        "        tweets[i] = tweets[i].strip()\n",
        "        tweet_len = len(tweets[i])\n",
        "        if tweet_len > 0:\n",
        "            if tweets[i][len(tweets[i]) - 1] == ':':\n",
        "                tweets[i] = tweets[i][:len(tweets[i]) - 1]\n",
        "\n",
        "        tweets[i] = tweets[i].replace('#', '')                                        # Remove any hash-tags symbols\n",
        "        tweets[i] = tweets[i].lower()                                                 # Convert every word to lowercase\n",
        "        tweets[i] = tweets[i].translate(str.maketrans('', '', string.punctuation))    # remove punctuations\n",
        "        tweets[i] = \" \".join(tweets[i].split())                                       # trim extra spaces\n",
        "        list_of_tweets.append(tweets[i].split(' '))                                   # convert each tweet from string type to as list<string> using \" \" as a delimiter\n",
        "\n",
        "    f.close()\n",
        "    return list_of_tweets"
      ],
      "metadata": {
        "id": "ermSFZBoDi4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform K-means clustering on the resulting tweets using at least 5 different values of K and report your results in the format below\n",
        "K is the number of clusters and mi is the centroid of the ith cluster."
      ],
      "metadata": {
        "id": "0Sz22lJFFRmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def k_means(tweets, k=4, max_iterations=50):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    # initialization, assign random tweets as centroids\n",
        "    count = 0\n",
        "    hash_map = dict()\n",
        "    while count < k:\n",
        "        random_tweet_idx = rd.randint(0, len(tweets) - 1)\n",
        "        if random_tweet_idx not in hash_map:\n",
        "            count += 1\n",
        "            hash_map[random_tweet_idx] = True\n",
        "            centroids.append(tweets[random_tweet_idx])\n",
        "\n",
        "    iter_count = 0\n",
        "    prev_centroids = []\n",
        "\n",
        "    # run the iterations until not converged or until the max iteration in not reached\n",
        "    while (is_converged(prev_centroids, centroids)) == False and (iter_count < max_iterations):\n",
        "\n",
        "        #print(\"running iteration \" + str(iter_count))\n",
        "\n",
        "        # assignment, assign tweets to the closest centroids\n",
        "        clusters = assign_cluster(tweets, centroids)\n",
        "\n",
        "        # to check if k-means converges, keep track of prev_centroids\n",
        "        prev_centroids = centroids\n",
        "\n",
        "        # update, update centroid based on clusters formed\n",
        "        centroids = update_centroids(clusters)\n",
        "        iter_count = iter_count + 1\n",
        "\n",
        "    if (iter_count == max_iterations):\n",
        "        print(\"max iterations reached, K means not converged\")\n",
        "    else:\n",
        "        print(\"\")\n",
        "\n",
        "    sse = compute_SSE(clusters)\n",
        "\n",
        "    return clusters, sse\n",
        "\n",
        "\n",
        "def is_converged(prev_centroid, new_centroids):\n",
        "\n",
        "    # false if lengths are not equal\n",
        "    if len(prev_centroid) != len(new_centroids):\n",
        "        return False\n",
        "\n",
        "    # iterate over each entry of clusters and check if they are same\n",
        "    for c in range(len(new_centroids)):\n",
        "        if \" \".join(new_centroids[c]) != \" \".join(prev_centroid[c]):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def assign_cluster(tweets, centroids):\n",
        "\n",
        "    clusters = dict()\n",
        "\n",
        "    # for every tweet iterate each centroid and assign closest centroid to a it\n",
        "    for t in range(len(tweets)):\n",
        "        min_dis = math.inf\n",
        "        cluster_idx = -1;\n",
        "        for c in range(len(centroids)):\n",
        "            dis = getDistance(centroids[c], tweets[t])\n",
        "            # look for a closest centroid for a tweet\n",
        "\n",
        "            if centroids[c] == tweets[t]:\n",
        "                # print(\"tweet and centroid are equal with c: \" + str(c) + \", t\" + str(t))\n",
        "                cluster_idx = c\n",
        "                min_dis = 0\n",
        "                break\n",
        "\n",
        "            if dis < min_dis:\n",
        "                cluster_idx = c\n",
        "                min_dis = dis\n",
        "\n",
        "        # randomise the centroid assignment to a tweet if nothing is common\n",
        "        if min_dis == 1:\n",
        "            cluster_idx = rd.randint(0, len(centroids) - 1)\n",
        "\n",
        "        # assign the closest centroid to a tweet\n",
        "        clusters.setdefault(cluster_idx, []).append([tweets[t]])\n",
        "        # print(\"tweet t: \" + str(t) + \" is assigned to cluster c: \" + str(cluster_idx))\n",
        "        # add the tweet distance from its closest centroid to compute sse in the end\n",
        "        last_tweet_idx = len(clusters.setdefault(cluster_idx, [])) - 1\n",
        "        clusters.setdefault(cluster_idx, [])[last_tweet_idx].append(min_dis)\n",
        "\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def update_centroids(clusters):\n",
        "\n",
        "    centroids = []\n",
        "\n",
        "    # iterate each cluster and check for a tweet with closest distance sum with all other tweets in the same cluster\n",
        "    # select that tweet as the centroid for the cluster\n",
        "    for c in range(len(clusters)):\n",
        "        min_dis_sum = math.inf\n",
        "        centroid_idx = -1\n",
        "\n",
        "        # to avoid redundant calculations\n",
        "        min_dis_dp = []\n",
        "\n",
        "        for t1 in range(len(clusters[c])):\n",
        "            min_dis_dp.append([])\n",
        "            dis_sum = 0\n",
        "            # get distances sum for every of tweet t1 with every tweet t2 in a same cluster\n",
        "            for t2 in range(len(clusters[c])):\n",
        "                if t1 != t2:\n",
        "                    if t2 < t1:\n",
        "                        dis = min_dis_dp[t2][t1]\n",
        "                    else:\n",
        "                        dis = getDistance(clusters[c][t1][0], clusters[c][t2][0])\n",
        "\n",
        "                    min_dis_dp[t1].append(dis)\n",
        "                    dis_sum += dis\n",
        "                else:\n",
        "                    min_dis_dp[t1].append(0)\n",
        "\n",
        "            # select the tweet with the minimum distances sum as the centroid for the cluster\n",
        "            if dis_sum < min_dis_sum:\n",
        "                min_dis_sum = dis_sum\n",
        "                centroid_idx = t1\n",
        "\n",
        "        # append the selected tweet to the centroid list\n",
        "        centroids.append(clusters[c][centroid_idx][0])\n",
        "\n",
        "    return centroids\n",
        "\n",
        "\n",
        "def getDistance(tweet1, tweet2):\n",
        "\n",
        "    # get the intersection\n",
        "    intersection = set(tweet1).intersection(tweet2)\n",
        "\n",
        "    # get the union\n",
        "    union = set().union(tweet1, tweet2)\n",
        "\n",
        "    # return the jaccard distance\n",
        "    return 1 - (len(intersection) / len(union))\n",
        "\n",
        "\n",
        "def compute_SSE(clusters):\n",
        "\n",
        "    sse = 0\n",
        "    # iterate every cluster 'c', compute SSE as the sum of square of distances of the tweet from it's centroid\n",
        "    for c in range(len(clusters)):\n",
        "        for t in range(len(clusters[c])):\n",
        "            sse = sse + (clusters[c][t][1] * clusters[c][t][1])\n",
        "\n",
        "    return sse\n",
        "\n"
      ],
      "metadata": {
        "id": "QPSBqXsyDt_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    datatxt = 'bbchealth.txt'\n",
        "    tweets = pre_process_tweets(datatxt)\n",
        "\n",
        "    # default number of experiments to be performed\n",
        "    kplus = 5\n",
        "\n",
        "    # default value of K for K-means\n",
        "    k = 3\n",
        "\n",
        "    # for every experiment 'e', run K-means\n",
        "    for e in range(kplus):\n",
        "\n",
        "        print(\"No. \" + str((e + 1)))\n",
        "        print(\"K = \" + str(k))\n",
        "\n",
        "        clusters, sse = k_means(tweets, k)\n",
        "\n",
        "        # for every cluster 'c', print size of each cluster\n",
        "        for c in range(len(clusters)):\n",
        "            print(str(c+1) + \": \", str(len(clusters[c])) + \" tweets\")\n",
        "\n",
        "\n",
        "        print(\"SSE : \" + str(sse))\n",
        "        print('\\n')\n",
        "\n",
        "        # increment k after every experiment\n",
        "        k = k + 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVRsPMKiFMol",
        "outputId": "10cdfff0-9fd6-4a67-e8b5-388243799888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. 1\n",
            "K = 3\n",
            "\n",
            "1:  1412 tweets\n",
            "2:  1507 tweets\n",
            "3:  1010 tweets\n",
            "SSE : 3391.3437963066544\n",
            "\n",
            "\n",
            "No. 2\n",
            "K = 5\n",
            "\n",
            "1:  1379 tweets\n",
            "2:  619 tweets\n",
            "3:  880 tweets\n",
            "4:  593 tweets\n",
            "5:  458 tweets\n",
            "SSE : 3323.0554131000804\n",
            "\n",
            "\n",
            "No. 3\n",
            "K = 7\n",
            "\n",
            "1:  526 tweets\n",
            "2:  470 tweets\n",
            "3:  743 tweets\n",
            "4:  229 tweets\n",
            "5:  333 tweets\n",
            "6:  882 tweets\n",
            "7:  746 tweets\n",
            "SSE : 3266.6444096414034\n",
            "\n",
            "\n",
            "No. 4\n",
            "K = 9\n",
            "\n",
            "1:  776 tweets\n",
            "2:  311 tweets\n",
            "3:  497 tweets\n",
            "4:  412 tweets\n",
            "5:  699 tweets\n",
            "6:  356 tweets\n",
            "7:  428 tweets\n",
            "8:  185 tweets\n",
            "9:  265 tweets\n",
            "SSE : 3214.4781595903637\n",
            "\n",
            "\n",
            "No. 5\n",
            "K = 11\n",
            "\n",
            "1:  222 tweets\n",
            "2:  890 tweets\n",
            "3:  152 tweets\n",
            "4:  174 tweets\n",
            "5:  362 tweets\n",
            "6:  557 tweets\n",
            "7:  249 tweets\n",
            "8:  371 tweets\n",
            "9:  321 tweets\n",
            "10:  455 tweets\n",
            "11:  176 tweets\n",
            "SSE : 3236.2937325783814\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}